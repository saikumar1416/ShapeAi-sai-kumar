In [1]:
from keras.datasets import mnist
data = mnist.load_data()
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
In [4]:
((X_train,y_train), (X_test, y_test)) = data
In [5]:
X_train = X_train.reshape((X_train.shape[0], 28*28)).astype('float32')
X_test = X_test.reshape((X_test.shape[0], 28*28)).astype('float32')
In [6]:
X_train = X_train / 255
X_test = X_test / 255
In [7]:
from keras.utils import np_utils

print(y_test.shape)

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

num_classes = y_test.shape[1]
print(y_test.shape)
(10000,)
(10000, 10)
In [8]:
from keras.models import Sequential
from keras.layers import DenseIn [10]:
model = Sequential()
model.add(Dense(32, input_dim = 28*28, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))
In [11]:
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
In [12]:
model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 32)                25120     
_________________________________________________________________
dense_1 (Dense)              (None, 64)                2112      
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 27,882
Trainable params: 27,882
Non-trainable params: 0
_________________________________________________________________
In [13]:
model.fit(X_train, y_train, epochs=10, batch_size=100)Epoch 1/10
600/600 [==============================] - 2s 2ms/step - loss: 0.8369 - accuracy: 0.7523
Epoch 2/10
600/600 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9386
Epoch 3/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1537 - accuracy: 0.9542
Epoch 4/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9644
Epoch 5/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1063 - accuracy: 0.9678
Epoch 6/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0939 - accuracy: 0.9718
Epoch 7/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9754
Epoch 8/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0714 - accuracy: 0.9787
Epoch 9/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0671 - accuracy: 0.9794
Epoch 10/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0594 - accuracy: 0.9821
Out[13]:
<tensorflow.python.keras.callbacks.History at 0x7fdeb37276d0>
In [14]:
scores = model.evaluate(X_test, y_test)
print(scores)
313/313 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9679
[0.10669098049402237, 0.9678999781608582]
